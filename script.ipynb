{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "# from pdfminer.high_level import extract_text\n",
    "import PyPDF2\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import glob\n",
    "import pycountry\n",
    "import time\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "import pymupdf4llm             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdfs(folder_path, text_folder_path):\n",
    "    # Loop through all the files in the folder\n",
    "    i = 0\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(folder_path, filename)\n",
    "            print(pdf_path)\n",
    "            # Extract text from the PDF using pymupdf4llm.to_markdown()\n",
    "            md_text = pymupdf4llm.to_markdown(pdf_path)\n",
    "\n",
    "            # clean the text by deleting extra \\n\n",
    "            # cleaned_md_text = re.sub(r'\\n\\n', '\\n', md_text)\n",
    "            \n",
    "            # Save the extracted text to a .txt file\n",
    "            text_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "            text_file_path = os.path.join(text_folder_path, text_filename)\n",
    "            \n",
    "            with open(text_file_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "                text_file.write(md_text)\n",
    "            \n",
    "            print(f\"Extracted text from {filename} to {text_filename}\")\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text_from_pdf(pdf_path, text_folder_path):\n",
    "    md_text = pymupdf4llm.to_markdown(pdf_path)\n",
    "\n",
    "    # clean the text by deleting extra \\n\n",
    "    # cleaned_md_text = re.sub(r'\\n\\n', '\\n', md_text)\n",
    "    \n",
    "    # Save the extracted text to a .txt file\n",
    "    filename = os.path.basename(pdf_path)\n",
    "    text_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "    text_file_path = os.path.join(text_folder_path, text_filename)\n",
    "    \n",
    "    with open(text_file_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "        text_file.write(md_text)\n",
    "    \n",
    "    print(f\"Extracted text from {filename} to {text_filename}\")\n",
    "\n",
    "    return text_file_path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_section(text, text_list):\n",
    "    original_list = [[] for i in range(7)]\n",
    "    # remove/split explanatory notes (repeated probable associates)\n",
    "    split_text = re.split(r'(EXPLANATORY NOTES)', text, maxsplit=1)\n",
    "    new_text = split_text[0]\n",
    "    break_pattern = r'(COMPANY INFORMATION|SHARE CAPITAL|PROBABLE ASSOCIATES|CURRENT APPOINTMENT HOLDERS|CURRENT SHAREHOLDERS|PAST APPOINTMENT HOLDERS|PAST SHAREHOLDERS|CURRENT CHARGES|PAST CHARGES|EXPLANATORY NOTES)'\n",
    "    sections = re.split(break_pattern, new_text)\n",
    "    sections = [section.strip() for section in sections if section.strip()]\n",
    "\n",
    "    for i in range(len(sections)):\n",
    "        section = sections[i]\n",
    "        if i % 2 == 0:\n",
    "            j = int(i/2 - 1)\n",
    "            # print(f'one section:\\n {section} \\n finsihed! \\n' )\n",
    "            if -1 < j <7:\n",
    "                original_list[j].append(section)\n",
    "                # print(f'{j}th section:\\n{section}\\n')\n",
    "                if -1 < j < 3:\n",
    "                    text_list[j].append(section)\n",
    "            # print(f'{j}th of textlist:\\n {section}')\n",
    "\n",
    "    part3_1 = part3_2 = part4_1 = part4_2 = part5_1 = part5_2 = ''\n",
    "    pattern = r'Disclosure(.*?)Date(.*)'\n",
    "    match = re.search(pattern, original_list[3][-1], re.DOTALL)\n",
    "    if match:\n",
    "        part3_1 = match.group(2)\n",
    "        # print(f'part3_1:{part3_1}')\n",
    "    \n",
    "    pattern = r'(.*?)Name(.*?)Disclosure(.*?)Date(.*)'\n",
    "    match = re.search(pattern, original_list[4][-1], re.DOTALL) \n",
    "    if match:\n",
    "        part3_2 = match.group(1).strip() if match.group(1) else print(\"Part 3_2 not found\")\n",
    "        part4_1 = match.group(4).strip() if match.group(4) else print(\"Part 4_1 not found\")\n",
    "        # print(f'part4_1:{part4_1}')\n",
    "    \n",
    "    pattern = r'(.*?)Name(.*?)Disclosure(.*?)Date(.*)'\n",
    "    match = re.search(pattern, original_list[5][-1], re.DOTALL)\n",
    "    if match:\n",
    "        part4_2 = match.group(1).strip() if match.group(1) else print(\"Part 4_2 not found\")\n",
    "        part5_1 = match.group(4).strip() if match.group(4) else print(\"Part 5_1 not found\")\n",
    "        # print(f'part5_1:{part5_1}')\n",
    "\n",
    "    pattern = r'(.*?)Name(.*?)Disclosure(.*?)Date(.*)'\n",
    "    match = re.search(pattern, original_list[6][-1], re.DOTALL)\n",
    "    if match:\n",
    "        part5_2 = match.group(1).strip() if match.group(1) else print(\"Part 5_2 not found\")\n",
    "\n",
    "    part3 = part3_1 + '\\n' + part3_2\n",
    "    part4 = part4_1 + '\\n' + part4_2\n",
    "    part5 = part5_1 + '\\n' + part5_2\n",
    "\n",
    "    text_list[3].append(part3.strip())\n",
    "    text_list[4].append(part4.strip())\n",
    "    text_list[5].append(part5.strip())\n",
    "    \n",
    "\n",
    "    return text_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_from_pdf(folder_path, text_folder_path):\n",
    "    file_list = []\n",
    "    company_info_list = []\n",
    "    share_captical_list = []\n",
    "    prob_associates_list = []\n",
    "    curr_appt_holder_list = []\n",
    "    curr_share_holder_list = []\n",
    "    past_appt_holder_list = []\n",
    "    text_list = [company_info_list, share_captical_list, prob_associates_list, curr_appt_holder_list, curr_share_holder_list, past_appt_holder_list]\n",
    "\n",
    "    i = 0\n",
    "    for filename in os.listdir(folder_path):\n",
    "        i += 1\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            file_list.append(filename)\n",
    "            pdf_path = os.path.join(folder_path, filename)\n",
    "            text_file_path = extract_text_from_pdf(pdf_path, text_folder_path)\n",
    "\n",
    "            with open(text_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                txt = file.read()\n",
    "\n",
    "            # remove the page front\n",
    "            txt = re.sub('#### ENTITY PROFILE REPORT', '', txt)\n",
    "            lines = txt.split('\\n') \n",
    "            third_line = lines[2]\n",
    "            txt = re.sub('\\n'+third_line, '', txt)\n",
    "            txt = re.sub(r'\\n{3,}', '\\n', txt)\n",
    "\n",
    "            with open(text_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(txt)\n",
    "\n",
    "            # cannot separate sections: as long as there is heading, it is missed\n",
    "            # seems like things before ### CURRENT APPOINTMENT HOLDERS is normal\n",
    "            text_list = separate_section(txt, text_list)\n",
    "\n",
    "        print(f'Finish {i}th file {filename}!\\n')\n",
    "\n",
    "        # trail\n",
    "        # if i > 3:\n",
    "        #     break\n",
    "\n",
    "    return text_list, file_list\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_path = \"1st 1000\"\n",
    "text_folder_path = 'new_txt'\n",
    "text_list, file_list = txt_from_pdf(folder_path,text_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('text_list.pkl', 'wb') as file:\n",
    "    pickle.dump(text_list, file)\n",
    "\n",
    "with open('file_list.pkl','wb') as file:\n",
    "    pickle.dump(file_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_list.pkl', 'rb') as file:\n",
    "    text_list = pickle.load(file)\n",
    "    \n",
    "print(text_list[3][4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_info_list = []\n",
    "share_captical_list = []\n",
    "prob_associates_list = []\n",
    "curr_appt_holder_list = []\n",
    "curr_share_holder_list = []\n",
    "past_appt_holder_list = []\n",
    "past_share_holder_list = []\n",
    "text_list = [company_info_list, share_captical_list, prob_associates_list, curr_appt_holder_list, curr_share_holder_list, past_appt_holder_list]\n",
    "\n",
    "original_list =[[] for i in range(7)]\n",
    "print(original_list)\n",
    "def separate_section(text, file_list):\n",
    "    original_list = [[] for i in range(7)]\n",
    "    # remove/split explanatory notes (repeated probable associates)\n",
    "    split_text = re.split(r'(EXPLANATORY NOTES)', text, maxsplit=1)\n",
    "    new_text = split_text[0]\n",
    "    break_pattern = r'(COMPANY INFORMATION|SHARE CAPITAL|PROBABLE ASSOCIATES|CURRENT APPOINTMENT HOLDERS|CURRENT SHAREHOLDERS|PAST APPOINTMENT HOLDERS|PAST SHAREHOLDERS|CURRENT CHARGES|PAST CHARGES|EXPLANATORY NOTES)'\n",
    "    sections = re.split(break_pattern, new_text)\n",
    "    sections = [section.strip() for section in sections if section.strip()]\n",
    "\n",
    "    for i in range(len(sections)):\n",
    "        section = sections[i]\n",
    "        if i % 2 == 0:\n",
    "            j = int(i/2 - 1)\n",
    "            # print(f'one section:\\n {section} \\n finsihed! \\n' )\n",
    "            if -1 < j <7:\n",
    "                original_list[j].append(section)\n",
    "                # print(f'{j}th section:\\n{section}\\n')\n",
    "                if -1 < j < 3:\n",
    "                    file_list[j].append(section)\n",
    "            # print(f'{j}th of textlist:\\n {section}')\n",
    "\n",
    "    # re-organize \n",
    "    # CURRENT APPOINTMENT HOLDER (index = 3)\n",
    "\n",
    "    pattern = r'Disclosure(.*?)Date(.*)'\n",
    "    match = re.search(pattern, original_list[3][-1], re.DOTALL)\n",
    "    if match:\n",
    "        part3_1 = match.group(2)\n",
    "        # print(f'part3_1:{part3_1}')\n",
    "    \n",
    "    pattern = r'(.*?)Name(.*?)Disclosure(.*?)Date(.*)'\n",
    "    match = re.search(pattern, original_list[4][-1], re.DOTALL) \n",
    "    if match:\n",
    "        part3_2 = match.group(1).strip() if match.group(1) else print(\"Part 3_2 not found\")\n",
    "        part4_1 = match.group(4).strip() if match.group(4) else print(\"Part 4_1 not found\")\n",
    "        # print(f'part4_1:{part4_1}')\n",
    "    \n",
    "    pattern = r'(.*?)Name(.*?)Disclosure(.*?)Date(.*)'\n",
    "    match = re.search(pattern, original_list[5][-1], re.DOTALL)\n",
    "    if match:\n",
    "        part4_2 = match.group(1).strip() if match.group(1) else print(\"Part 4_2 not found\")\n",
    "        part5_1 = match.group(4).strip() if match.group(4) else print(\"Part 5_1 not found\")\n",
    "        # print(f'part5_1:{part5_1}')\n",
    "\n",
    "    pattern = r'(.*?)Name(.*?)Disclosure(.*?)Date(.*)'\n",
    "    match = re.search(pattern, original_list[6][-1], re.DOTALL)\n",
    "    if match:\n",
    "        part5_2 = match.group(1).strip() if match.group(1) else print(\"Part 5_2 not found\")\n",
    "\n",
    "    part3 = part3_1 + '\\n' + part3_2\n",
    "    part4 = part4_1 + '\\n' + part4_2\n",
    "    part5 = part5_1 + '\\n' + part5_2\n",
    "\n",
    "    file_list[3].append(part3.strip())\n",
    "    file_list[4].append(part4.strip())\n",
    "    file_list[5].append(part5.strip())\n",
    "    \n",
    "\n",
    "    return file_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing: remove **, Entity Report\n",
    "def get_first_and_last_non_signal_index(text):\n",
    "    first_match = re.search(r'[A-Za-z0-9]', text)\n",
    "    last_match = None\n",
    "    for match in re.finditer(r'[A-Za-z0-9]', text):\n",
    "        last_match = match\n",
    "    first_index = first_match.start() if first_match else None\n",
    "    last_index = last_match.start() if last_match else None\n",
    "    \n",
    "    return first_index, last_index\n",
    "\n",
    "\n",
    "def clean_3(text):\n",
    "    text = re.sub(r'#{2,}', r'\\n', text)\n",
    "    text = re.sub(r'\\*\\*ENTITY PROFILE REPORT\\*\\*', r'\\n', text)\n",
    "    text = re.sub(r'-{2,}', r'\\n', text)\n",
    "    # remove newline (inconsistency in separating different information)\n",
    "    text = re.sub(r'\\n{2,}', '; ', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    first_index, last_index = get_first_and_last_non_signal_index(text)\n",
    "    if first_index:\n",
    "        text = text[first_index:].strip()\n",
    "    if last_index:\n",
    "        text = text[:last_index+1].strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_list[4][980])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text_list[3][120]\n",
    "print(text)\n",
    "text = clean_3(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_replace(text, pattern):\n",
    "    # Function to replace matched patterns with an empty string and extract the content\n",
    "    def replacer(match):\n",
    "        # Extract the text inside **...** and join parts with space if multiline\n",
    "        extracted = ' '.join([part if part is not None else '' for part in match.groups()]).strip()\n",
    "        extracted_texts.append(extracted)\n",
    "        # Return an empty string to replace the matched pattern\n",
    "        return ';'\n",
    "    \n",
    "    extracted_texts = []  # List to hold the extracted text\n",
    "    modified_text = re.sub(pattern, replacer, text)  # Replace matched patterns with ''\n",
    "    \n",
    "    return extracted_texts, modified_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_name_3(text):\n",
    "    pattern = r'\\*\\*(.*?)\\*\\*(?:\\n*\\s*\\*\\*(.*?)\\*\\*)?'\n",
    "    extracted_texts, modified_text = extract_and_replace(text, pattern)\n",
    "    num = len(extracted_texts)\n",
    "    modified_text = re.sub('\\*', '', modified_text)\n",
    "    return extracted_texts, modified_text, num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names, modified_text, num = extract_name_3(text)\n",
    "print(\"Extracted Names:\", names)\n",
    "# print(\"Modified Text:\", modified_text)\n",
    "print(num)\n",
    "print(modified_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new: extract dates (use slices to be more efficient)\n",
    "# to so: how to order the dates\n",
    "def extract_dates_3(text, num):\n",
    "    \n",
    "    slices = text.split(';')\n",
    "    slices = [s.strip() for s in slices if s]\n",
    "\n",
    "    pattern = r'\\b\\d{1,2} [A-Z][a-z]{2} \\d{4}\\b'\n",
    "    date_slice = [[]for s in slices]\n",
    "\n",
    "    for i, s in enumerate(slices):\n",
    "        dates = re.findall(pattern, s)\n",
    "        slices[i] = ';' + re.sub(pattern, '', s) + ';'\n",
    "        date_slice[i] = dates\n",
    "\n",
    "    flattened_dates = [date for sublist in date_slice for date in sublist]\n",
    "    date_list = flattened_dates\n",
    "\n",
    "    assert num == int(len(date_list )/2), 'number of extracted dates not equal to number of names!'\n",
    "    remaining_text = ';'.join(slices)\n",
    "    \n",
    "    return date_list, remaining_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list, remaining_text = extract_dates_3(modified_text, num)\n",
    "print(date_list)\n",
    "print(remaining_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new: extract ID info first\n",
    "# two list: one is the list they are together, another the opposite. then combine them in right order.\n",
    "def extract_ID_3(text, num):\n",
    "    # id_list = []\n",
    "    id_type_list = []\n",
    "    id_number_list = []\n",
    "    extract_id = 0\n",
    "    \n",
    "    slices = text.split(';')\n",
    "    slices = [s.strip() for s in slices if s]\n",
    "    # print(f'slices:{slices}')\n",
    "    \n",
    "    types = ['NRIC (Citizen)', 'Passport/Oth ers', 'ACRA Registration Number', 'FIN', 'NRIC (Permanent Resident)']\n",
    "    pre_types = ['NRIC (C', 'Pass',  'ACRA', 'FIN', 'NRIC (P']\n",
    "    # may have empty spaces which can not be extracted by full type name (if delete all empty spaces too early will obstruct the rest procedures)\n",
    "    end_types = ['zen)', 'thers', 'mber', 'FIN', 'dent)']\n",
    "\n",
    "    # reformat: separate id infos\n",
    "    indexes = []\n",
    "    add_indexes = []\n",
    "    for t in types:\n",
    "        escaped_type = re.escape(t)\n",
    "        matches = re.finditer(escaped_type, text)\n",
    "        for match in matches:\n",
    "            indexes.append(match.start())  \n",
    "            indexes.append(match.end())\n",
    "    indexes = sorted(indexes)\n",
    "    # print('indexes:')\n",
    "    # print(indexes)\n",
    "    for x in range(1,len(indexes)-1, 2):\n",
    "        if x+1 < len(indexes) and indexes[x+1] - indexes[x] >2: # turning point (different sets of id info)\n",
    "            add_indexes.append(indexes[x])\n",
    "    if add_indexes:\n",
    "        for idx in add_indexes:\n",
    "            text_1 = text[:idx] + \";\" + text[idx:]\n",
    "        text = text_1\n",
    "        slices = text.split(';')\n",
    "        slices = [s.strip() for s in slices if s]\n",
    "\n",
    "    # print(slices)\n",
    "\n",
    "    # extract slice with id types and count type number\n",
    "    type_dict = {key:value for key, value in zip(pre_types, types)}\n",
    "    type_end_dict = {key:value for key, value in zip(pre_types, end_types)}\n",
    "    types_pattern = '|'.join(re.escape(t) for t in pre_types)\n",
    "    # pattern = rf'([0-9A-Z]+\\s*({types_pattern}))'\n",
    "    type_start_idx_list = [[] for i in slices]\n",
    "    type_num_list = [[] for i in slices]\n",
    "\n",
    "    # Q: what if different id info in one slice\n",
    "    for i,s in enumerate(slices):\n",
    "        match_iter = re.finditer(types_pattern , s)\n",
    "        match_iter= list(match_iter)\n",
    "        if match_iter:\n",
    "            type_num = len(match_iter)\n",
    "            type_num_list[i] = type_num\n",
    "            type_start_idx = match_iter[0].start()\n",
    "            type_start_idx_list[i] = type_start_idx\n",
    "\n",
    "            # extract exact ID types\n",
    "            text_after = s[type_start_idx:]\n",
    "            id_types = []\n",
    "            for m,match in enumerate(match_iter):\n",
    "                pre_type = match.group()\n",
    "                id_type = type_dict[pre_type]\n",
    "                id_types.append(id_type)\n",
    "                if m == type_num - 1:\n",
    "                    type_ending = type_end_dict[pre_type]\n",
    "                    idx = s[match.start():].find(type_ending)\n",
    "                    if idx != -1:\n",
    "                        idx = idx + len(type_ending)\n",
    "                        text_after = s[match.start():][idx:] + ';'\n",
    "                        # print(f'text_after:{text_after}')\n",
    "                    else:\n",
    "                        print(r'\\nCannot find the full ID type (last one)!')\n",
    "                        print(s)\n",
    "                        print(f\"Searching for type_ending: '{type_ending}' in text_after: {s[match.start():]}\")\n",
    "\n",
    "            id_type_list.append(' '.join(id_types))\n",
    "\n",
    "            # extract Id number according to num of types and separate \n",
    "            text_ahead = s[:type_start_idx].strip()\n",
    "            texts = re.split(r'\\s+', text_ahead)\n",
    "            # print(f'texts:\\n{texts}')\n",
    "            if type_num <= len(text):\n",
    "                potential_id_number = texts[-type_num:]\n",
    "                # print(f'potential_id_number:{potential_id_number}')\n",
    "                if potential_id_number:\n",
    "                    # check id numbers\n",
    "                    pattern = re.compile(r'^[A-Z0-9]+$')\n",
    "                    all_valid = all(pattern.match(item) for item in potential_id_number)\n",
    "                    if all_valid:\n",
    "                        id_number = ' '.join(potential_id_number).strip()\n",
    "                        id_number_list.append(id_number)\n",
    "                        text_ahead = ';' + ' '.join(texts[:len(texts)- type_num]) \n",
    "                        # print(f'text_ahead:{text_ahead}')\n",
    "                    else:\n",
    "                        print(\"Some extracted id numbers are not in the correct format:\")\n",
    "                        print(potential_id_number)\n",
    "                \n",
    "\n",
    "            else:\n",
    "                print(f'Incomplete {i}th slice: {s}!')\n",
    "    \n",
    "            slices[i] = text_ahead + ' ' + text_after\n",
    "            # print(f'{i}th slice: {slices[i]}')\n",
    "\n",
    "        # no id type in this slice, but still may have id number\n",
    "        # else:\n",
    "            \n",
    "    # print(id_number_list)\n",
    "    # print(id_type_list)\n",
    "\n",
    "    type_num_list = [i for i in type_num_list if i!=[]]\n",
    "    remaining_text = ';'.join(slices)\n",
    "    if len(id_number_list) == num:\n",
    "        assert len(id_type_list) == num, 'Number of Id info not equal to number of names!'\n",
    "        \n",
    "    else:\n",
    "        print('Likely ID number and Id type are not read together!')\n",
    "        extract_id = 1\n",
    "        \n",
    "\n",
    "    return id_number_list, id_type_list, remaining_text, extract_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_number_list, id_type_list, remaining_text, extract_id, type_num_list = extract_ID_3(modified_text, num)\n",
    "print(id_number_list)\n",
    "print(id_type_list)\n",
    "print(remaining_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new: extract Nation and position\n",
    "\n",
    "def extract_nation_posi_3(text, num):\n",
    "    # appear only once (no mateer whether with multiple id infos)\n",
    "    nation_list = []\n",
    "    posi_list = []\n",
    "\n",
    "    slices = text.split(';')\n",
    "    slices = [s.strip() for s in slices if s]\n",
    "    # print(slices)\n",
    "\n",
    "    nation_posi_slice = [[] for s in slices]\n",
    "    nation_slice = [[] for s in slices]\n",
    "    posi_slice = [[] for s in slices]\n",
    "    pattern = r'([A-Z]+)\\s([A-Z][a-z]+(?:\\s[A-Z][a-z]+)*)'\n",
    "    for i, s in enumerate(slices):\n",
    "        nation_posis = re.findall(pattern, s)\n",
    "        if nation_posis:\n",
    "            slices[i] = ';' + re.sub(pattern, '', s) + ';'\n",
    "            nation_posi_slice[i] = nation_posis\n",
    "            \n",
    "            nations, posis = zip(*nation_posis)\n",
    "            nation_slice[i] = list(nations)\n",
    "            posi_slice[i] = list(posis)\n",
    "    \n",
    "    nation_list = [nation for sublist in nation_slice for nation in sublist]\n",
    "    posi_list = [posi for sublist in posi_slice for posi in sublist]\n",
    "    # print(nation_list)\n",
    "    # print(posi_list)\n",
    "\n",
    "    assert len(nation_list) == len(posi_list) == num, 'number of extracted nationalities and positions not equal to number of names!'\n",
    "\n",
    "    remaining_text = ';'.join(slices)\n",
    "    return nation_list, posi_list, remaining_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nation_list, posi_list, remaining_text = extract_nation_posi_3(remaining_text, num)\n",
    "print(nation_list)\n",
    "print(posi_list)\n",
    "print(remaining_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new: extract address (all got left)\n",
    "# Q: whether need further check?\n",
    "def extract_addr_3(text, num, extract_id):\n",
    "    id_no_list = []\n",
    "    slices = text.split(';')\n",
    "    slices = [s.strip() for s in slices if s and any(char.isdigit() for char in s)]\n",
    "    # print(slices)\n",
    "    if extract_id == 0:\n",
    "        assert len(slices) == num, 'number of slices not equal to number of names!'\n",
    "        address_list = slices\n",
    "    else:\n",
    "        # extract address and ID number separately\n",
    "        # texts = [s.split(' ') for s in slices]\n",
    "        # len_text = [len(t) for t in texts]\n",
    "        id_texts = [s for s in slices if len(s.split(' '))<5]\n",
    "        address_list = [s for s in slices if len(s.split(' '))>=5]\n",
    "        # id_num_list = [len(t.split(' ')) for t in id_texts]\n",
    "        # assert id_num_list == type_num_list, 'Number of ID No. not equal to number of ID type!'\n",
    "        id_no_list = id_texts\n",
    "\n",
    "    return address_list, id_no_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_list = extract_addr_3(remaining_text, num)\n",
    "print(address_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def organize_id_no(id_no_list_1, id_no_list_2, origin_text, num):\n",
    "    id_no_list = []\n",
    "    if id_no_list_2:\n",
    "        id_no_list_combined = id_no_list_1 + id_no_list_2\n",
    "        id_no_indexes = [origin_text.find(no) for no in id_no_list_combined]\n",
    "        if -1 not in id_no_indexes:\n",
    "            id_no_dict = {key:value for key,value in zip(id_no_indexes,id_no_list_combined)}\n",
    "            id_indexes_sorted = id_no_indexes.copy()\n",
    "            id_indexes_sorted.sort(reverse=False)\n",
    "            for idx in id_indexes_sorted:\n",
    "                id_no = id_no_dict[idx]\n",
    "                id_no_list.append(id_no)    \n",
    "        else:\n",
    "            print('Cannot find several id number in the given text!')\n",
    "    else:\n",
    "        id_no_list = id_no_list_1\n",
    "    \n",
    "    assert len(id_no_list) == num, 'Number of ID No not equals to number of names!'\n",
    "    \n",
    "    return id_no_list\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CURRENT APPOINTMENT HOLDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new for part 3: name -> id -> dates -> nation, posi -> address\n",
    "def curr_appt_hders(text):\n",
    "    name_list = id_no_list = id_type_list = date_list = nation_list = posi_list = address_list = []\n",
    "    \n",
    "    modified_text = clean_3(text)\n",
    "    # print(f'cleaned_text:\\n{modified_text}')\n",
    "    stripped_text = re.sub(r'[\\*\\s\\;]', '', modified_text)\n",
    "    # print(stripped_text)\n",
    "    if len(stripped_text) < 10:\n",
    "        name_list = id_no_list = id_type_list = date_list = nation_list = posi_list = address_list = ['-']\n",
    "        print('Empty current apppointment holder!')\n",
    "    else:\n",
    "        origin_text = modified_text\n",
    "        \n",
    "        name_list, remaining_text, num = extract_name_3(modified_text)\n",
    "        print(f'name_list:\\n{name_list}\\n')\n",
    "        \n",
    "        date_list, remaining_text = extract_dates_3(remaining_text, num)\n",
    "        print(f'date_list:\\n{date_list}\\n')\n",
    "\n",
    "        id_no_list_1, id_type_list, remaining_text, extract_id = extract_ID_3(remaining_text, num)\n",
    "        print(f'id_type_list:\\n{id_type_list}\\n')\n",
    "\n",
    "\n",
    "        nation_list, posi_list, remaining_text = extract_nation_posi_3(remaining_text, num)\n",
    "        print(f'nation_list:\\n{nation_list}\\n') \n",
    "        print(f'posi_list:\\n{posi_list}\\n') \n",
    "\n",
    "        address_list, id_no_list_2 = extract_addr_3(remaining_text, num, extract_id)\n",
    "        print(f'address_list:\\n{address_list}\\n') \n",
    "\n",
    "        id_no_list = organize_id_no(id_no_list_1, id_no_list_2, origin_text, num)\n",
    "        print(f'id_no_list:\\n{id_no_list}')\n",
    "    \n",
    "    # check the id_no_list and id_type list\n",
    "    # example: original extracted text 0244381C NRIC S0244381C (Citizen) NRIC (Citizen)\n",
    "    # normaly the id_type should be the same, maybe can just delete the id type in id_no_list\n",
    "    \n",
    "\n",
    "    return name_list, id_no_list, id_type_list, date_list, nation_list, posi_list, address_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 87\n",
    "text = text_list[3][idx]\n",
    "# print(text,'\\n')\n",
    "print(file_list[idx], '\\n')\n",
    "name_list, id_number_list, id_type_list, date_list, nation_list, posi_list, address_list = curr_appt_hders(text)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
